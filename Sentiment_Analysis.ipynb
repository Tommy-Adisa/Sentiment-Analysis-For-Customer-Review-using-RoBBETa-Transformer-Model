{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tommy-Adisa/Sentiment-Analysis-For-Customer-Review-using-RoBBETa-Transformer-Model/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a2b30e0",
      "metadata": {
        "id": "2a2b30e0"
      },
      "source": [
        "# IMPROVING BUSINESS DECISIONS THROUGH SENTIMENT ANALYSIS OF CUSTOMER REVIEWS USING NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7041180",
      "metadata": {
        "id": "f7041180"
      },
      "source": [
        "### LIBRARY IMPORTATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcea3216",
      "metadata": {
        "id": "bcea3216"
      },
      "outputs": [],
      "source": [
        "\n",
        "#pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ecbd06b",
      "metadata": {
        "scrolled": true,
        "id": "2ecbd06b"
      },
      "outputs": [],
      "source": [
        "#pip install pyspellchecker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a30d1ef8",
      "metadata": {
        "scrolled": true,
        "id": "a30d1ef8"
      },
      "outputs": [],
      "source": [
        "#pip install fsspec==2023.5.0 s3fs huggingface-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dd208bc",
      "metadata": {
        "id": "8dd208bc"
      },
      "outputs": [],
      "source": [
        "#pip install datasets pandas nltk scikit-learn seaborn matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "138fae2f",
      "metadata": {
        "id": "138fae2f"
      },
      "outputs": [],
      "source": [
        "pip install transformers[torch]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2f18353",
      "metadata": {
        "id": "f2f18353"
      },
      "outputs": [],
      "source": [
        "pip install accelerate -U\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af314141",
      "metadata": {
        "id": "af314141"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e373d3b",
      "metadata": {
        "id": "2e373d3b"
      },
      "outputs": [],
      "source": [
        "#pip install --upgrade torch typing_extensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b5029cb",
      "metadata": {
        "id": "7b5029cb"
      },
      "outputs": [],
      "source": [
        "#pip install --upgrade typing_extensions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f95a9e35",
      "metadata": {
        "scrolled": true,
        "id": "f95a9e35"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import contractions  # Import contractions library\n",
        "\n",
        "from spellchecker import SpellChecker #Import spellcheck library. this handles errors in spellings\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3033da",
      "metadata": {
        "id": "0d3033da"
      },
      "outputs": [],
      "source": [
        "# This is to downlod nltk resources\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14196978",
      "metadata": {
        "id": "14196978"
      },
      "outputs": [],
      "source": [
        "#This is to import data from the dataset\n",
        "\n",
        "df= pd.read_csv('Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "661220ad",
      "metadata": {
        "id": "661220ad"
      },
      "outputs": [],
      "source": [
        "#This is to check the first 7 head of the data\n",
        "df.head(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28c9987e",
      "metadata": {
        "id": "28c9987e"
      },
      "outputs": [],
      "source": [
        "#This is to output the dimension of a dataFrame\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeddf5f2",
      "metadata": {
        "id": "eeddf5f2"
      },
      "outputs": [],
      "source": [
        "#this is to show the descriptive statistics of the numerical columns of this dataFrame\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bad67724",
      "metadata": {
        "id": "bad67724"
      },
      "outputs": [],
      "source": [
        "#this is to get the information of the dataframe\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0436fd1a",
      "metadata": {
        "id": "0436fd1a"
      },
      "outputs": [],
      "source": [
        "# checking the columns\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be307e60",
      "metadata": {
        "id": "be307e60"
      },
      "source": [
        "###  Handling missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "593c373e",
      "metadata": {
        "id": "593c373e"
      },
      "outputs": [],
      "source": [
        "# Drop missing values in 'reviews.text' column\n",
        "df.dropna(subset=['reviews.text'], inplace=True)\n",
        "\n",
        "# Convert all reviews to string format\n",
        "df['reviews.text'] = df['reviews.text'].astype(str)\n",
        "\n",
        "# Check for missing values again\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "069ece19",
      "metadata": {
        "id": "069ece19"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=[\"reviews.didPurchase\", \"reviews.id\", \"reviews.numHelpful\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aadf8074",
      "metadata": {
        "id": "aadf8074"
      },
      "outputs": [],
      "source": [
        "df[\"reviews.doRecommend\"].fillna(\"unknown\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a411316f",
      "metadata": {
        "id": "a411316f"
      },
      "outputs": [],
      "source": [
        "######df[\"reviews.username\"].fillna(\"anonymous\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce5f4c43",
      "metadata": {
        "id": "ce5f4c43"
      },
      "outputs": [],
      "source": [
        "print(df.isnull().sum())  # Should return 0 missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2893451",
      "metadata": {
        "id": "d2893451"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "515164c4",
      "metadata": {
        "id": "515164c4"
      },
      "source": [
        "#### To ensure the accuracy of sentiment analysis, the raw text data will undergo preprocessing. This involves:\n",
        "\n",
        "    Text Cleaning: Removing special characters, punctuation, and unnecessary symbols.\n",
        "\n",
        "    Tokenisation: Splitting the text into individual words or phrases.\n",
        "\n",
        "    Lemmatization: Converting words into their base forms (convert to lowercase) this is to improve NLP efficiency.\n",
        "\n",
        "    Stopword Removal: Eliminating common words that do not contribute to sentiment (e.g., \"the,\" \"is,\" \"and\").\n",
        "    \n",
        "    Handling Contractions – Expand contractions (e.g., \"don't\" → \"do not\").\n",
        "\n",
        "    Handling Imbalanced Data: If necessary, balance the dataset by oversampling underrepresented classes or undersampling overrepresented ones\n",
        "\n",
        "    Handle misspellings or slang (optional).\n",
        "\n",
        "This step standardises the data, making it more suitable for computational analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42ef6d00",
      "metadata": {
        "id": "42ef6d00"
      },
      "source": [
        "### Text Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1b8112c",
      "metadata": {
        "id": "b1b8112c"
      },
      "source": [
        "#### The goal is to remove special characters, punctuation, and unnecessary symbols."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed4540aa",
      "metadata": {
        "id": "ed4540aa"
      },
      "outputs": [],
      "source": [
        "# Define the text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Remove non-alphabetical characters and make everything lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I)  # Remove digits, punctuation\n",
        "    text = contractions.fix(text) #Expand contractions\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() #this is to remove extral spaces\n",
        "\n",
        "    # Tokenize the text (split into words)\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatize each token\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Reconstruct the cleaned text\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to the 'reviews.text' column\n",
        "df['Cleaned_Review'] = df['reviews.text'].apply(preprocess_text)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "988d6f32",
      "metadata": {
        "id": "988d6f32"
      },
      "outputs": [],
      "source": [
        "# Display original vs cleaned text\n",
        "df[['reviews.text', 'Cleaned_Review']].head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95c5cb15",
      "metadata": {
        "id": "95c5cb15"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9c8529e8",
      "metadata": {
        "id": "9c8529e8"
      },
      "source": [
        "### Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07d571dc",
      "metadata": {
        "id": "07d571dc"
      },
      "source": [
        "I am comparing using VADER sentiment Analyzer and SentiWordNEt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57d3f770",
      "metadata": {
        "id": "57d3f770"
      },
      "outputs": [],
      "source": [
        "# This is to Initialize VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5cd1f60",
      "metadata": {
        "id": "c5cd1f60"
      },
      "outputs": [],
      "source": [
        "# Function to analyze sentiment using VADER\n",
        "def get_vader_sentiment(text):\n",
        "    score = sia.polarity_scores(str(text))['compound']\n",
        "    return 'Positive' if score > 0.05 else 'Negative' if score < -0.05 else 'Neutral'\n",
        "\n",
        "# Apply VADER sentiment analysis\n",
        "df['VADER_Sentiment'] = df['Cleaned_Review'].apply(get_vader_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14e3a7f1",
      "metadata": {
        "id": "14e3a7f1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad3befb",
      "metadata": {
        "id": "4ad3befb"
      },
      "outputs": [],
      "source": [
        "# Function to get SentiWordNet sentiment score\n",
        "def get_sentiwordnet_sentiment(text):\n",
        "    words = word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    pos_score = 0\n",
        "    neg_score = 0\n",
        "    count = 0\n",
        "\n",
        "    for word, tag in pos_tags:\n",
        "        wn_tag = get_wordnet_pos(tag)\n",
        "        if wn_tag:\n",
        "            synsets = list(swn.senti_synsets(word, wn_tag))\n",
        "            if synsets:\n",
        "                pos_score += synsets[0].pos_score()\n",
        "                neg_score += synsets[0].neg_score()\n",
        "                count += 1\n",
        "\n",
        "    if count == 0:\n",
        "        return 'Neutral'\n",
        "\n",
        "    avg_score = (pos_score - neg_score) / count\n",
        "    return 'Positive' if avg_score > 0.05 else 'Negative' if avg_score < -0.05 else 'Neutral'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e80f7cf4",
      "metadata": {
        "id": "e80f7cf4"
      },
      "outputs": [],
      "source": [
        "# Helper function to convert POS tags to WordNet POS\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Apply SentiWordNet sentiment analysis\n",
        "df['SentiWordNet_Sentiment'] = df['Cleaned_Review'].apply(get_sentiwordnet_sentiment)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "700f820f",
      "metadata": {
        "id": "700f820f"
      },
      "outputs": [],
      "source": [
        "# Display sentiment comparison\n",
        "sns.countplot(x='VADER_Sentiment', data=df, palette='coolwarm')\n",
        "plt.title('Sentiment Analysis using VADER')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c51a38da",
      "metadata": {
        "id": "c51a38da"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x='SentiWordNet_Sentiment', data=df, palette='coolwarm')\n",
        "plt.title('Sentiment Analysis using SentiWordNet')\n",
        "plt.show()\n",
        "\n",
        "# Save results\n",
        "df.to_csv('results/sentiment_analysis_comparison.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f2e63ad",
      "metadata": {
        "id": "3f2e63ad"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Display first few rows after sentiment analysis\n",
        "df[['reviews.text', 'Cleaned_Review', 'VADER_Sentiment', 'SentiWordNet_Sentiment']].head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cfad221",
      "metadata": {
        "id": "9cfad221"
      },
      "outputs": [],
      "source": [
        "# Machine Learning-Based Sentiment Classification\n",
        "# Convert categorical labels to numerical values\n",
        "df['Sentiment_Label'] = df['VADER_Sentiment'].map({'Positive': 1, 'Neutral': 0, 'Negative': -1})\n",
        "\n",
        "\n",
        "df['Sent_Label_SentiWordNEt'] = df['SentiWordNet_Sentiment'].map({'Positive': 1, 'Neutral': 0, 'Negative': -1})\n",
        "\n",
        "\n",
        "df[['reviews.text', 'Cleaned_Review', 'VADER_Sentiment', 'SentiWordNet_Sentiment', 'Sentiment_Label', 'Sent_Label_SentiWordNEt']].head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1157fc4",
      "metadata": {
        "id": "b1157fc4"
      },
      "outputs": [],
      "source": [
        "# Feature extraction using TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(df['Cleaned_Review'])\n",
        "y = df['Sentiment_Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0d6865c",
      "metadata": {
        "id": "d0d6865c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "243b8f16",
      "metadata": {
        "id": "243b8f16"
      },
      "source": [
        "#### Using GPT 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b6cacc9",
      "metadata": {
        "scrolled": true,
        "id": "7b6cacc9"
      },
      "outputs": [],
      "source": [
        "#pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d25002c",
      "metadata": {
        "id": "3d25002c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "beded593",
      "metadata": {
        "id": "beded593"
      },
      "source": [
        "### The goal is to Training my Own Sentiment Analysis Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96ea506d",
      "metadata": {
        "id": "96ea506d"
      },
      "source": [
        "### Steps to use\n",
        "#### We will start with a pre-trained model (like BERT or RoBERTa) and fine-tune it on this dataset.\n",
        "\n",
        "##### Steps:\n",
        "##### 1. Load a pretrained transformer model (BERT, RoBERTa).\n",
        "##### 2. Train the model using PyTorch or TensorFlow.\n",
        "##### 3. Evaluate accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3660765a",
      "metadata": {
        "id": "3660765a"
      },
      "outputs": [],
      "source": [
        "# Select the relevant column\n",
        "df = df[['reviews.text', 'reviews.rating']]\n",
        "\n",
        "# Drop missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Display some rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79489f6d",
      "metadata": {
        "id": "79489f6d"
      },
      "outputs": [],
      "source": [
        "def assign_sentiment(rating):\n",
        "    if rating >= 4:\n",
        "        return 1  # Positive\n",
        "    elif rating == 3:\n",
        "        return 0  # Neutral\n",
        "    else:\n",
        "        return -1  # Negative\n",
        "\n",
        "# Apply sentiment function\n",
        "df['sentiment'] = df['reviews.rating'].apply(assign_sentiment)\n",
        "\n",
        "# Keep only needed columns\n",
        "df = df[['reviews.text', 'sentiment']]\n",
        "\n",
        "# Show class distribution\n",
        "df['sentiment'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45fb0f9a",
      "metadata": {
        "id": "45fb0f9a"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"bert-base-uncased\"  # or use \"roberta-base\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_data(texts):\n",
        "    return tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "# Example of tokenization\n",
        "sample_text = df['reviews.text'][0:3].tolist()\n",
        "tokenized_sample = tokenize_data(sample_text)\n",
        "tokenized_sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e49e500e",
      "metadata": {
        "id": "e49e500e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a1c955a",
      "metadata": {
        "id": "9a1c955a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class AmazonReviewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize the text\n",
        "        encoding = self.tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create dataset instance\n",
        "dataset = AmazonReviewsDataset(df['reviews.text'].tolist(), df['sentiment'].tolist(), tokenizer)\n",
        "\n",
        "# Check sample\n",
        "dataset[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6660580a",
      "metadata": {
        "id": "6660580a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68dfd764",
      "metadata": {
        "id": "68dfd764"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Define dataset sizes\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# Split dataset\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9511a969",
      "metadata": {
        "id": "9511a969"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a975f4d",
      "metadata": {
        "id": "3a975f4d"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Load the model (3 classes: negative, neutral, positive)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c7f8680",
      "metadata": {
        "id": "2c7f8680"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d615932",
      "metadata": {
        "scrolled": false,
        "id": "2d615932"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Training settings\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1b9aee9",
      "metadata": {
        "id": "c1b9aee9"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb1d0c0b",
      "metadata": {
        "id": "fb1d0c0b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2ce4810",
      "metadata": {
        "id": "e2ce4810"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}